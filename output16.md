### Page1
ビジョンGNN。An Image is Worth Graph of NodesKai Han1,2∗Yunhe Wang2∗Jianyuan Guo2Yehui Tang2,3Enhua Wu1,41State Key Lab of Computer Science, ISCAS & UCAS2華為ノアズアークラボ3北京大学4University of Macau{kai.han,yunhe.wang}@huawei.com, weh@ios.ac.cnAbstractNetwork 構築が深い学習ベースのコンピューターヴィジョンシステムで鍵となる役割を果たす。広く使われている畳み込みニューラルネットワークと変換器は、画像をグリッドまたはシーケンス構造として扱うため、不規則で複雑なオブジェクトを捉えることは容易ではない。本論文では、画像をグラフ構造として表現することを提案し、視覚タスクのためのグラフレベルの特徴を抽出する新しいVision GNN (ViG) アーキテクチャを導入する。まず、画像をいくつかのパッチに分割し、それをノードとみなして、最近傍同士を接続してグラフを構成する。画像のグラフ表現に基づき、全てのノード間で情報を変換し交換するためのViGモデルを構築する。ViGは2つの基本的なモジュールから構成される。グラフ情報の集約と更新のためのグラフ畳み込みによるGrapherモジュールと、ノード特徴変換のための2つの線形層によるFFNモジュールである。ViGのアーキテクチャは、等方性とピラミッド型の両方が、異なるモデルサイズで構築されている。画像認識と物体検出タスクに関する広範な実験により、我々のViGアーキテクチャの優位性が実証された。PyTorch のコードは https://github.com/huawei-noah/Efficient-AI-Backbones で、MindSpore のコードは https://gitee.com/mindspore/models.1IntroductionIn で入手可能です。現代のコンピュータビジョンシステムでは、畳み込みニューラルネットワーク (CNN) がデファクトスタンダードなネットワークアーキテクチャとして使われています [29, 27, 17]。近年では，視覚タスクのための注目機構を備えた変換器が導入され[9, 3]，競争力のある性能を獲得しています．これらの進歩は、視覚モデルを前例のない高みへと押し上げている。図1に示すように、画像データは通常ユークリッド空間における画素の規則的な格子として表現される。CNN [29]は画像にスライディングウィンドウを適用し、シフト不変性と局所性を導入している。最近のビジョン変換器[9]やMLP[49]は，画像をパッチのシーケンスとして扱います．例えば，ViT [9]は，224×224の画像を16×16の多数のパッチに分割し，長さ196のシーケンスを形成して入力します．コンピュータビジョンの基本タスクの1つは，画像中のオブジェクトを認識することです．オブジェクトは通常、形状が不規則な四角形ではないため、ResNetやViTのような従来のネットワークで一般的に用いられてきたグリッドやシーケンス構造は冗長であり、処理に柔軟性がない。物体は部品の集合体として捉えることができ、例えば、人間は頭部、上半身、下半身に大別される＊Equal contribution.36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2206.00272v3 [cs.CV] 4 Nov 2022
### Page2
(a) グリッド構造 (b) シーケンス構造 (b) グラフ構造図1: 画像のグリッド、シーケンス、グラフ表現の説明。グリッド構造では、画素またはパッチは空間的な位置によってのみ並べられる。シーケンス構造では、2次元画像はパッチのシーケンスに変換される。グラフ構造では、ノードはその内容によってリンクされ、局所的な位置によって制約されることはない。このような関節で結ばれたパーツは、自然にグラフ構造を形成する。このグラフを解析することで、人物を認識することができる。また、グラフは一般化されたデータ構造であり、グリッドやシーケンスはグラフの特殊なケースと見なすことができる。画像のグラフ表現に基づき、視覚タスクのためのビジョングラフ ニューラルネットワーク（略称ViG）を構築する。画像パッチのグラフを構築した後、ViGモデルを用いて、全てのノード間で情報の変換と交換を行う。ViGの基本セルは2つの部分からなる。グラファとFFN（feed-forward network）モジュールである。Grapherモジュールは、グラフの畳み込みによる情報処理をベースに構築されている。従来のGNNの過平滑化現象を緩和するために、FFNモジュールはノードの特徴変換とノードの多様性を促進するために利用される。GrapherとFFNモジュールを用いて、ViGモデルを等方性とピラミッド型の両方で構築する。実験では、画像分類や物体検出などの視覚タスクにおいて、ViGモデルの有効性を実証する。例えば、ピラミッド型ViG-SはImageNetclassiﬁcationタスクにおいて82.1%のトップ1精度を達成し、代表的なCNN (ResNet [17]), MLP (CycleMLP [5]), transformer (Swin-T [35]) と同等のFLOPs (About 4.5G) を上回る性能を示しました。我々の知る限り、グラフニューラルネットワークを大規模な視覚タスクに適用することに成功したのは、我々の作品が初めてである。2関連研究このセクションでは、まずコンピュータビジョンにおけるバックボーンネットワークについて再考する。LeNet [29]に始まり、CNNは様々な視覚タスク、例えば画像分類[27]、物体検出[42]、意味的セグメンテーション[36]で成功裏に使用されています。CNNのアーキテクチャはここ10年で急速に発展している．代表的なものとして、ResNet [17]、MobileNet [21]、NAS [75, 70]がある。2020年から視覚タスクのために視覚変換器が導入された[14, 9, 3, 4]。主な改良点としては，ピラミッドアーキテクチャ[57, 35]，局所的注意[15, 35]，位置符号化[61]などが挙げられます．特別に設計されたモジュール[5, 32, 12, 48]により，MLPは競争力のある性能を達成し，物体検出やセグメンテーションといった一般的な視覚タスクで機能することができます．
### Page3
2.2Graph Neural Network最も初期のグラフニューラルネットワークは[11, 44]で概説され たものである。近年では、空間ベースのGCNのバリエーションが紹介されており、[39, 1, 10]などがある。スペクトルベースのGCNは、スペクトルグラフ理論に基づくグラフ畳み込みを導入したBrunaら[2]によって最初に発表された。以来，スペクトルベース GCN を改良・拡張する研究が数多く提案されている [18, 7, 26]．GCN は通常、ソーシャルネットワーク [13]、引用ネットワーク [45]、生化学グラフ [55] などのグラフデータに適用されます。コンピュータビジョン分野 [63, 28, 56, 25] における GCN の応用には、主に点群の分類、シーングラフ生成、行動認識などがあります。点群とは、通常、LiDARスキャンによって収集された空間内の3次元点の集合です。GCN は点群の分類とセグメンテーションのために研究されてきた[28, 58, 69]。シーングラフの生成は、オブジェクトとその関係を持つグラフを入力画像から解析することを目的としており、通常、オブジェクト検出器とGCNを組み合わせることで解決されます[63, 68]。GCNは、人間の関節の自然なグラフを処理することで、人間の動作認識タスクに利用されました[24, 67]．GCNは、自然に形成されたグラフを用いた特殊な視覚的タスクにしか取り組むことができない。3アプローチ本節では、画像をグラフに変換する方法を説明し、視覚表現を学習するためのビジョンGNNアーキテクチャを紹介する。各パッチを特徴ベクトルxi∈RDに変換すると、X = [x1, x2, - - , xN] （Disは特徴次元、i = 1, 2, - - , N）となり、これらの特徴はV = {v1, v2, - -, vN}と表される不規則ノードの集合として見ることができる。各ノードviについて、そのK個の最近傍N(vi)を求め、全てのvj∈N(vi)についてvjからviへ向けられた辺ejiを追加する。ここで、Eは全ての辺を表す。以下、グラフの構築過程をG = G(X)と表記する。画像をグラフデータとしてとらえ、その表現を抽出するためにGNNを利用する方法を検討する。1) グラフは一般化されたデータ構造であり、グリッドやシーケンスはグラフの特殊なケースとみなすことができる。2) 画像中の物体は通常四角形ではなく、その形状は不規則であるため、グラフはグリッドやシーケンスよりも複雑な物体のモデル化に柔軟である。3）物体は部品の構成として見ることができ（例えば、人間は頭、上半身、腕、脚に大別される）、グラフ構造はそれらの部品間の接続を構築できる。4）GNNの先進的な研究は、視覚タスクに取り組むために移植できる。グラフレベルの処理一般論として、特徴X∈RN×Dから始める。一般に、特徴量 X∈RN×D から出発し、まず、特徴量に基づくグラフを構築する。G = G(X)とする。グラフ畳み込み層は、近傍ノードの特徴を集約してノード間の情報交換を行うことができる。具体的には、グラフ畳み込み3
### Page4
G′ = F(G, W)= Update(Aggregate(G, Wagg), Wupdate),(1)where Wagg と Wupdate はそれぞれ集約と更新操作の学習可能な重みである。より具体的には、集約操作は近傍ノードの特徴を集約してノードの表現を計算し、更新操作はさらに集約した特徴を統合する：x′i = h(xi, g(xi, N(xi), Wagg), Wupdate),(2)where N(xli) is the set of neighbors nodes of xli.このとき、近傍ノードの集合はxliとなる。ここでは、簡単かつ効率的な最大相対グラフ畳み込み[30]を採用する：g(-) = x′i = [xi, max({xj - xi|j ∈ N(xi)}],(3)h(-) = x′i = x′′i Wupdate,(4) ただしバイアス項を省略したものである。以上のグラフレベルの処理をX′＝GraphConv（X）と表すことができる。さらに、グラフ畳み込みのマルチヘッド更新演算を導入する。集約された特徴量x′iは、まず、h個のヘッド、すなわち、head1, head2, - - , headhに分割され、これらのヘッドは、それぞれ異なる重みで更新される。(5)マルチヘッド更新操作により、複数の表現空間の情報を更新することができ、特徴の多様性に有利である。Layer0.00.10.20.30.40.50.6DiversityVanilla ResGCNOur ViGFigure 3: Feature Diversity of nodes aslayer changes.ViG block従来のGCNは、グラフデータの特徴を集約して抽出するために、通常、数層のグラフ畳み込みを繰り返し行っている。図3では、∥X - 1〜xT ∥として測定され、∥x = arg min〜x ∥ X -1 〜xT ∥ [8]として示されているように、深いGCNs [31, 40] の過平滑化現象は、ノード特徴の明確さを減少させ、視覚認識の性能低下につながるでしょう。この問題を解決するために、ViGblockでは、より多くの特徴変換と非線形活性化を導入する。グラフ畳み込みの前後に線形層を適用し、ノード特徴を同じドメインに投影し、特徴の多様性を向上させる。グラフ畳み込みの後に非線形活性化関数を挿入し、層の崩壊を回避する。実際には、入力特徴 X∈RN×D が与えられれば、Grapher モジュールは Y=σ(GraphConv(XWin))Wout + X,(6)where Y∈RN×D, Win と Wout は完全連結層の重み、σは活性化関数、例,さらに、特徴変換能力を高め、過平滑化現象を緩和するために、各ノードにフィードフォワードネットワーク(FFN)を利用している。FFNモジュールは2つの完全連結層を持つ単純な多層パーセプトロンである：Z = σ(Y W1)W2 + Y,(7)where Z∈RN×D, W1 and W2 is the weights of fully-connected layers, and the bias term isomitted.FFNモジュールは、2つの完全連結層を持つ多層パーセプトロンである。FFNの隠れ次元は通常Dより大きい。GrapherモジュールとFFNモジュールの両方において、すべての完全連結層またはグラフ畳み込み層の後にバッチ正規化が適用され、式6と7では簡潔になるように省略されている。GrapherモジュールとFFNモジュールのスタックがViGブロックを構成し、ネットワークを構築するための基本的な構成単位となる。画像のグラフ表現と提案するViGブロックに基づき、図2に示すような視覚タスクのためのViGネットワークを構築することができる。vanilla ResGCN [30]と比較して、我々のViGは、層が深くなっても特徴の多様性を維持し（図3）、識別的な表現を学習することができる4。
### Page5
3.2ネットワークアーキテクチャ コンピュータビジョンの分野において、一般的に使用されている変換器は通常等方性アーキテクチャ（例：ViT [9]）であり、CNNはピラミッドアーキテクチャ（例：ResNet [17]）を用いることが好ましいとされる。等方性アーキテクチャとは、ViT[9]やResMLP[50]のように、ネットワーク全体が同じ大きさと形を持つ特徴量を持つことを意味する。モデルサイズの異なる3種類の等方性ViGアーキテクチャ、すなわち、ViG-Ti, S, Bを構築する。受容野を徐々に拡大するため、これら3つのモデルでは、層が深くなるにつれて近傍ノード数Kが9から18へと直線的に増加する。ヘッド数はデフォルトでash = 4とした。表1：等方性ViGアーキテクチャのバリエーション。FLOPsは解像度224×224の画像に対して計算されている。モデル深さ寸法DParams (M)FLOPs (B)ViG-Ti121927.11.3ViG-S1632022.74.5ViG-B1664086.817.7Pyramid architecture：ResNet[17]やPVT[57]など、階層が深くなるほど空間サイズが徐々に小さくなる特徴を抽出し、画像のマルチスケール特性を考慮する。このように、我々はピラミッド型ViGモデルの先進的な設計を利用し、4つのバージョンのピラミッド型ViGモデルを構築した。その詳細を表2に示す。表2.ピラミッド型ViGモデルの詳細設定Pyramid ViGシリーズの詳細設定。D: 特徴次元，E: FFNにおける隠れ次元比率，K: GCNにおける近傍数，H × W: 入力画像サイズ．Tiは極小、Sは小、Mは中、Bは基本を表す。StageOutput sizePyramidViG-TiPyramidViG-SPyramidViG-MPyramidViG-BStemH4 × W4Conv×3Conv×3Conv×3Stage 1H4 × W4D = 48E = 4K = 9×2D = 80E = 4K = 9×2D = 96E = 4K = 9×2D = 128E = 4K = 9×2DownsampleH8 × W8Conv×3Conv×3D = 4K = 9×2Conv×3D = 48E = 9×2D = 4K = 4K = 4K = 9×2Conv×3Conv = 3Conv×3D = 3ComW8ConvConvStage 2H8 × W8D = 96E = 4K = 9×2D = 160E = 4K = 9×2D = 192E = 4K = 9×2D = 256E = 9×2DownsampleH16 × W16ConvConvConvStage3H16 × W16D = 240E = 4K = 9×6D = 400E = 4K = 9×6D = 384E = 4K = 9×16D = 512E = 4K = 9×18DownsampleH32 × W32ConvConvStage 4H32 × W32D = 384E = 4K = 4K = 9×18DownsampleH32 = W32ConvConvStage= 384E = 4K = 9×2D = 640E = 4K = 9×2D = 768E = 4K = 9×2D = 1024E = 4K = 9×2Head1 × 1Pooling & MLPPooling & MLPPooling & MLPParameters (M)10.727.351.792.6FLOPs (B)1.74.68.916.8Positional encoding.In order to represent the position information of the nodes, add a positionalencoding vector to each node feature:xi ← xi + ei,(8)where ei ∈ RD.ノードの位置情報を表現するため，位置エンコードベクトルを各ノードの特徴量に付加する．式8の絶対位置符号化は、iostropicとpyramidの両アーキテクチャで適用される。ピラミッド型ViGでは、さらに相対位置エンコーディングを5
### Page6
は、Swin Transformer [35]のような先進的な設計に準拠している。4.1 データセットと実験設定データセット画像分類タスクにおいては、広く用いられているベンチマークImageNet ILSVRC 2012 [43]を以下の実験において用いる。ImageNetは120万枚の学習画像と5万枚の検証画像から構成され、1000のカテゴリに分類されています。ImageNet データセットのライセンスは http://www.image-net.org/download を参照。物体検出には、80の物体カテゴリを持つCOCO 2017 [34]データセットを使用します。COCO 2017 には 118K の学習画像と 5K の検証画像が含まれている．これらのデータセットのライセンスについては、https://cocodataset.org/#home.Table 3: Training hyper-parameters for ImageNet を参照されたい。(ピラミッド）ViGTiSMBEpochs300OptimizerAdamW [37]Batch size1024Start learning rate (LR)2e-3Learning rate scheduleCosineWarmup epochs20Weight decay0.05Label smoothing [47]0.1Stochastic path [22]0.10.10.3Repeated augment [20]✓RandAugment [6]✓Mixup prob.を設定した場合、学習確率は0.8となる。[73]0.8Cutmix prob.[72]1.0ランダム消去prob.[また、GELU[19]を式6と式7の非線形活性化関数として用いる。ImageNet分類については、公平な比較のためにDeiT[51]で提案された一般的な学習戦略を用いる。データ拡張には、RandAugment [6], Mixup [73], Cutmix [72], random erasing [74], repeated augment [20]がある。詳細は表3に示す。COCO検出タスクでは、検出フレームワークとしてRetinaNet [33]とMaskR-CNN [16]を用い、バックボーンとして我々のPyramid ViGを使用する。すべてのモデルはCOCO 2017のトレーニングセットで "1×"スケジュールで学習され、検証セットで評価される。我々は、PyTroch [41]とMindSpore [23]を使用してネットワークを実装し、8つのNVIDIA V100 GPUですべてのモデルを訓練します。表4：ImageNetでのViGと他の等方性ネットワークの結果。♠ CNN, ■ MLP, ♦ Transformer,⋆ GNN.ModelResolutionParams (M)FLOPs (B)Top-1Top-5♠ ResMLP-S12 conv3x3 [50]224×22416.73.277.3.0♠ ConvMixer-768/32 [52]224×22421.120.980.2♠ ConvMixer-1536/20 [52]224×22451.651.481.4♦ ViT-B/16 [9]384×38486.455.577.9♦ DeiT-Ti [51]224×2245.71.372.291.2♦ ViT-B/16 [9]236×383.7.41◆DeiT-S[51]224×22422.14.679.895.0◆DeiT-B [51]224×22486.417.681.895.7■ ResMLP-S24 [50]224×224306.079.494.5■ ResMLP-B24 [50]224×22411623.081.095.1◆DiMLP-S24 [51]224×224326.7.074.5◆ResMLP-S24[51] [50]224×224326.080.70■ミキサーB/16 [49]224×2245911.776.4⋆ ViG-Ti（当方）224×2247.11.373.992.0⋆ ViG-S （当方）224×22422.74.580.495.2⋆ ViG-B （当方）224×22486.817.782.395.96(当方）224×226×226×227.8117.772.774.
### Page7
4.2 ImageNetでの主な結果Isotropic ViGostropicアーキテクチャを用いたニューラルネットワークは、特徴量の大きさを計算本体内で変更しないため、拡張が容易で、ハードウェアアクセラレーションに適しています。この方式は自然言語処理の変換モデルで広く利用されています[53]。また、ConvMixer [49], ViT [9], ResMLP [50]など、最近の視覚分野のニューラルネットもこの方式を採用している。表4では、我々の等方性ViGと既存の等方性CNN [50, 49]、変換器[9, 51]、MLP [50, 49]を比較する。この結果から、ViGは他の種類のネットワークよりも性能が高いことがわかる。例えば、我々のViG-Tiは73.9%のトップ1精度を達成し、同様の計算コストでDeiT-Tiモデルより1.7%高い。ピラミッドViGピラミッドアーキテクチャは、ネットワークの深化とともに特徴マップの空間サイズを徐々に縮小し、画像のスケール不変の特性を利用してマルチスケール特徴を生成することができる。ResNet [17]、SwinTransformer [35]、CycleMLP [5]などの先進的なネットワークは、通常ピラミッド型アーキテクチャを採用している。表5では、我々のPyramid ViGとこれらの代表的なピラミッド型ネットワークを比較している。その結果、CNN、MLP、transformerといった最新のピラミッド型ネットワークと比較して、Pyramid ViGは凌駕もしくは同程度の性能を持つことが分かりました。表5：ImageNetにおけるPyramid ViGと他のピラミッド型ネットワークとの比較結果。CNN, ■ MLP, ♦Transformer, ⋆ GNN.ModelResolutionParams (M)FLOPs (B)Top-1Top-5♠ ResNet-18 [17, 59]224×224121.870.689.7♠ ResNet-50[17, 59]224×22425.64.179.895.0♠ ResNet-152 [17, 59]224×22460.211.581.895.9♠ BoTNet-T3 [46]224×22433.57.381.7♠ BoTNet-T3 [46]224×22454.710.982.8♠ BoTNet-T3 [46]256×25675.119.383.5♦ PVT-Tiny [57]224×22413.21.975.1♦ PVT-Small [57]224×22424.53.879.8♦ PVT-Medium [57]224×22444.26.781.2♦ PVT-Large [57]224×22461.49.881.7♦ CvT-13 [60]224×224204.581.6♦ CvT-21 [60]224×224327.171.999.999.999.999.999.999.999.999.999.999.999.999.999.999.999.999.999.999182.5♦ CvT-21 [60]384×3843224.983.3♦ Swin-T [35]224×224294.581.395.5♦ Swin-S [35]224×224508.783.096.2♦ Swin-B [35]224×2248815.483.596.5■ CycleMLP-B2 [5]224×224273.483.5♦ サイクルマリンパッド [35]224×223226.3♦ Swin-B [35]224×22382.5981.6■ CycleMLP-B3 [5]224×224386.982.4■ CycleMLP-B4 [5]224×2245210.183.0■ Poolformer-S12 [71]224×224122.077.293.5■ Poolformer-S36 [71]224×224315.281.495.2■ CycleMLP-B3 [5]224×226．5■Poolformer-M48 [71]224×2247311.982.596.0⋆ Pyramid ViG-Ti (当社)224×22410.71.778.294.2⋆ Pyramid ViG-S（当社）224×22427.34.682.196.0⋆ Pyramid ViG-M (当社）224×22451.78.983.196.4⋆ Pyramid ViG-B (ours)224×22492.616.883.796.54.3Ablation StudyImageNet分類課題に対して提案手法のアブレーション実験を行い，基本アーキテクチャとして等方性ViG-Tiを用いた．グラフコンボリューションの種類EdgeConv [58], GIN [64], GraphSAGE [13] と Max-Relative GraphConv [30] というグラフコンボルーションの典型例をテストしている．表6より、様々なグラフ畳み込みのトップ1精度は、DeiT-Ti,7よりも優れていることがわかる。
### Page8
表6：グラフコンボリューションの種類によるImageNetの結果。GraphConvParams (M)FLOPs (B)Top-1EdgeConv [58]7.22.474.3GIN [64]7.01.372.8GraphSAGE [13]7.31.674.0Max-Relative GraphConv [30]7.11.373.9 ViGアーキテクチャが柔軟なことを表していると考えられる．これらのうち、Max-RelativeはFLOPsと精度のトレードオフが最も良い。グラフニューラルネットワークを視覚タスクに適応させるため、GrapherモジュールにFC層を導入し、FFNブロックを特徴変換に利用する。これらのモジュールの効果をアブレーションスタディで評価した。また、各モデルのFLOPが同じになるように特徴量を変更し、公平に比較した。表7より、画像分類にグラフ畳み込みを直接利用した場合、性能が低いことがわかる。表7：ImageNetにおけるViGのモジュールの効果GrapherにおけるGraphConvFC moduleFFN moduleParams (M)FLOPs (B)Top-1���5.グラフの構築において、近傍ノードの数Kは集計範囲を制御するハイパーパラメータであり、近傍ノードの数が少なすぎると情報量が減少する。近傍ノード数が少なすぎると情報伝達が悪くなり、多すぎると過度な平滑化が起こる。Kを3から20の間で調整し、その結果を表8に示す。表8：ImageNetにおけるTop-1精度 vs K.K3691215209 to 18Top-172.273.473.673.673.573.373.9head 数：マルチヘッド更新操作によりGrapherモジュールが異なる部分空間におけるノード特徴を処理することができるようになる．式5におけるヘッド数hは、部分空間における変換多様性とFLOPsを制御する。hを1〜8まで調整し、結果を表9に示す。表9：ImageNetにおけるトップ1精度とhの関係h12468FLOPs / Top-11.6B / 74.21.4B / 74.01.3B / 73.91.2B / 73.71.2B / 73.74.4Object Detection我々は、オブジェクト検出課題に本モデルの適用し、その汎化を評価した．この際、RetinaNet[33]とMask R-CNN [16]の検出フレームワークのバックボーンとして、ImageNetで事前に学習したPyramid ViG-Sを利用する。これらのモデルは、一般的に使用されている「1x」スケジュールで学習され、1280×800の入力サイズでFLOPsが計算される。表10より、我々のPyramid ViG-Sは、RetinaNet[17]、CycleMLP[5]、Swin Transformer[35]などの異なるタイプの代表的なバックボーンよりもRetinaNetとMaskR-CNNにおいて優れた性能を示していることが分かる。この優れた結果は、ViGアーキテクチャの汎化能力を示している。
### Page9
表 10：COCO val2017 での物体検出とインスタンス分割の結果。我々のPyramid ViGはRetinaNetとMask R-CNNフレームワークで他のバックボーンと比較している。BackboneRetinaNet 1×ParamFLOPsmAPAP50AP75APSAPMAPLResNet50 [17]37.7M239.3B36.355.338.619.340.048.8ResNeXt-101-32x4d [62]56.4M319B39.5ResNeXt-32x4d [62]46.6M319B39.4M32.4M32.5M32.3M32.3M32.3B32.3B32.4M32.5M32.3M32.3B42.5B31.5B31959.642.722.344.252.5PVT-Small [57]34.2M226.5B40.461.344.225.042.955.7CycleMLP-B2 [5]36.5M230.9B40.661.443.222.944.454.5Swin-T [35]38.5M244.8B41.562.144.225.144.955.5Pyramid ViG-S (ours)36.2M240.0B41.863.144.728.545.453.4BackboneMask R-CNN 1×ParamFLOPsAPbAPb50APb75APmAPm50APm75ResNet50 [17]44.2M260.1B38.058.641.434.455.136.7PVT-Small [57]44.1M245.1B40.462.943.837.860.140.3CycleMLP-B2 [5]46.5M249.5B42.164.045.738.961.241.8PoolFormer-S24 [71]41.0M40.162.243.437.059.139.6Swin-T [35]47.8M264.0B42.264.646.239.161.642.0Pyramid ViG-S （当社）45.8M258.8B42.665.246.039.462.441.645可視化我々の ViG モデルについて理解深めるために，構築した ViG-S のグラフ構造 を可視化している．図4は、深さの異なる2つのサンプル（1ブロック目と12ブロック目）のグラフを示したものである。五芒星が中心ノードで、同じ色のノードはその近傍ノードである。すべてのエッジを描画すると煩雑になるため、2つの中心ノードを可視化した。このモデルにより、コンテンツに関連するノードを1次近傍として選択することができることがわかる。浅い層では、色やテクスチャなどの低レベルかつ局所的な特徴に基づいて近傍ノードが選択される傾向がある。深層部では、中心ノードの近傍は、より意味的で同じカテゴリに属するノードが選択される。このように、ViGnetworkは、ノードの内容や意味表現によって徐々にノードを結びつけ、より良い物体認識を助けることができる。五芒星は中心ノード、同色のノードはグラフ内の隣接ノードである9。
### Page10
5結論本研究では、画像をグラフデータとして表現し、グラフニューラルネットワークを視覚タスクに活用する研究を開拓している。画像を複数のパッチに分割し、それらをノードとして捉え、これらのノードを基にグラフを構築することで、不規則で複雑な物体をより適切に表現することができる。画像のグラフ構造に対してグラフ畳み込みを直接用いると、過平滑化の問題があり、性能が低下する。そこで、各ノードの内部にさらに特徴変換を導入し、情報の多様性を促進する。画像のグラフ表現と改良されたグラフブロックに基づき、等方性及びピラミッド型アーキテクチャのビジョンGNN (ViG) ネットワークを構築する。画像認識と物体検出の広範な実験により、提案したViGアーキテクチャの優位性を実証する。我々は、ビジョンGNNに関するこの先駆的な研究が、一般的な視覚タスクに対する基本的なアーキテクチャとして役立つことを期待している。 謝辞この研究は、NSFC (62072449, 61872345), National Key R&D Program of China (2021YFB1715800), and Macau Science &Tech.基金(0018/2019/ACP)の支援を受けている。この研究に使用した MindSpore、CANN (Compute Architecture for Neural Networks)、Ascend AIProcessor の支援に感謝する。参考文献[1] James Atwood and Don Towsley.Diffusion-convolutional neural networks.In NIPS, pages 2001-2009,2016.[2] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun.グラフ上のスペクトルネットワークと局所的に接続されたネットワーク。変換器によるエンドツーエンドのオブジェクト検出。4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,Chao Xu, and Wen Gao.In ECCV, pages 213-229, 2020.事前学習された画像処理変換器．5] Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo.Cyclemlp:mlp-like architecture fordense prediction.Randaugment:を用いた実用的な自動データオーグメンテーションシステム．また、このような場合にも、「曖昧さ」を解消するために、「曖昧さ」を解消するための工夫が必要です。グラフ上の畳み込みニューラルネットワークと高速な局所スペクトルフィルタリング.NIPS, volume 29, 2016 に掲載。[8] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.アテンションは必要なものばかりではない。純粋なアテンションは深さによって二重指数的にランクを失う。ICML, pages 2793-2803 に収録。9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.「画像は16x16語の価値がある。画像認識のための変換器.10] Justin Gilmer, Samuel Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.量子化学のためのニューラル・メッセージパッシング。ICML、ページ1263-1272にて。PMLR, 2017.[11] Marco Gori, Gabriele Monfardini, and Franco Scarselli.グラフドメインにおける学習のための新しいモデル。InIJCNN, volume 2, pages 729-734, 2005.[12] Jianyuan Guo, Yehui Tang, Kai Han, Xinghao Chen, Han Wu, Chao Xu, Chang Xu, and Yunhe Wang.Hire-mlp.Vision mlp via hierarchive reholder, and Hire-mlp:階層的再配置を介したビジョン mlp．また、このような場合にも、「曖昧さ」を解消するために、「曖昧さ」 の解消が必要である．大規模グラフにおける帰納的表現学習．InNIPS, pages 1025-1035, 2017.[14] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,Chunjing Xu, Yixing Xu, et al. A survey on vision transformer.IEEE Transactions on Pattern Analysisand Machine Intelligence, 2022.[15] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.また，このような場合，「曖昧さ」を解消するために，「曖昧さ」を解消するために，「曖昧さ」を解消するために，「曖昧さ」を解消するために，「曖昧さ」を解消するために，「曖昧さ」を解消するために，「曖昧さ」を解消する必要がある．マスクr-cnn.In ICCV, pages 2961-2969,2017.10
### Page11
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition.In CVPR, pages 770-778, 2016.[18] Mikael Henaff, Joan Bruna, and Yann LeCun.Deep convolutional networks on graph-structured data.arXiv preprint arXiv:1506.05163, 2015.[19] Dan Hendrycks and Kevin Gimpel.Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,2016.[20] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry.Augment yourbatch:インスタンスの繰り返しによる汎化の改善．また、このような漠然とした疑問や不安を解消するため に、「漠然とした疑問や不安を解消する方法」を提案する。Mobilenets:Efﬁcient convolutional neural networks for mobile visionapplications. arXiv preprint arXiv:1704.04861, 2017.[22] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger.Deep networks with stochasticdepth.In ECCV, pages 646-661.Springer, 2016.[23] Huawei.Mindspore. https://www.mindspore.cn/, 2020.[24] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena.Structural-rnn:このような場合，「時空間グラフ」に対する深層学習が必要となる．In CVPR, pages 5308-5317, 2016.[25] Yongcheng Jing, Yining Mao, Yiding Yang, Yibing Zhan, Mingli Song, Xinchao Wang, and Dacheng Tao.Learning graph neural networks for image style transfer.グラフニューラルネットワークの学習による画像スタイル転送。ECCV, 2022.[26] Thomas N Kipf and Max Welling.Semi-supervised classiﬁcation with graph convolutional networks.InICLR, 2017.[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet classiﬁcation with deep convolutionalneural networks.また、このような場合、「曖昧さ」を解消するため に、「曖昧さ」解消の方法を検討する必要がある。また、このような場合、「曖昧さ」を解消するために、「曖昧さ」を解消した上で、「曖昧さ」を解消する方法を検討する必要がある。In CVPR, pages 4558-4567, 2018.[29] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.Gradient-based learning applied todocument recognition（勾配に基づく学習の文書認識への適用）.Proceedings of the IEEE, 86(11):2278-2324, 1998.[30] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem.Deepgcns:Can gcns go as deep as cnns?In ICCV, pages 9267-9276, 2019.[31] Qimai Li, Zhichao Han, and Xiao-Ming Wu.Deeper insights into graph convolutional networks forsemi-supervised learning.In AAAI, pages 3538-3545, 2018.[32] Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao.As-mlp:An axial shifted mlp architecture forvision.このような場合、"as-mlp "は、"as-mlp "と "as-mlp "の間に位置する。Focal loss for dense objectdetection.In ICCV, 2017.[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,and C Lawrence Zitnick.Microsoft coco:Microsoft coco: Common objects in context.In ECCV, pages 740-755, 2014.[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.Swintransformer: Hierarchical Vision Transformer using Shifted Window.また、このような場合にも、「曖昧さ」の解消が必要である。セマンティックセグメンテーションのための完全畳み込みネットワーク．In CVPR, 2015.[37] Ilya Loshchilov and Frank Hutter.Decoupled weight decay regularization.arXiv preprintarXiv:1711.05101, 2017.[38] Alessio Micheli.グラフのためのニューラルネット。文脈構成的アプローチ.IEEE Transactions onNeural Networks, 20(3):498-511, 2009.[39] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.このような場合、「曖昧さ」を回避するために、"曖昧さ "と "曖昧さ "の間の距離を縮める必要がある。ICML, pages 2014-2023 にて。PMLR, 2016.[40] 大野健太, 鈴木泰治.グラフニューラルネットワークはノードクラッシフィケーションの表現力を指数関数的に低下させる.このような場合、「曖昧さ」を解消することが重要である。An imperative style, high-performance deeplearning library.NeurIPS, 2019.[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-cnn: Towards real-time object detectionwith region proposal networks.In NIPS, pages 91-99, 2015.11
### Page12
[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International Journal of Computer Vision, 115(3):211-252, 2015.[44] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.Thegraph neural network model.また、このような場合にも、「曖昧さ」の解消を図るため、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」の解消を図るべく、「曖昧さ」を解消することを提案した。CVPR, pages 16519-16529, 2021.[47] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.The Bottleneck transformers for visual recognition.Rethinking theinception architecture for computer vision.In CVPR, 2016.[48] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang.An image patch isa wave:位相を考慮したビジョン mlp.また，このような画像は，画像処理におい ても有効である．視覚のための全MLPアーキテクチャ。50] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave,Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al.「レスムルプ」:また、このような画像分類のためのフィードフォワードネットワー クは、データ効率的な学習が可能である。アテンションによるデータ効率の良い画像変換と蒸留の訓練。ICML, 2021.[52] Asher Trockman and J Zico Kolter.パッチはすべて必要ですか？ arXiv preprint arXiv:2201.09792, 2022.[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ŁukaszKaiser, and Illia Polosukhin.Attention is all you need.NeurIPS, 2017.[54] Aladin Virmaux and Kevin Scaman.Lipschitz regularity of deep neural networks: analysis and efﬁcientestimation.In NeurIPS, pages 3839-3848, 2018.[55] Nikil Wale, Ian A Watson, and George Karypis.化学化合物の検索と分類のための記述子空間の比較.このような場合，"nothing "であっても，"nothing "であれば，"nothing "であっても，"nothing "であれば，"nothing "であっても，"nothing "でなければならない．また、このような場合にも、「曖昧さ」を解消するために、「曖昧さ」の解消を図る。In ICCV, pages 3056-3065, 2019.[57] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, andLing Shao.ピラミッドビジョントランスフォーマー。また、このような場合にも、「俯瞰的な視点」を持つことが重要である。Acm Transactions On Graphics (tog), 38(5):1-12, 2019.[59] Ross Wightman, Hugo Touvron, and Hervé Jégou.（ロス・ワイトマン、ヒューゴ・トゥブロン、エルヴェ・ジェグー）。Resnetの逆襲。An improved training procedure intimm. arXiv preprint arXiv:2110.00476, 2021.[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.Cvt:Introducing convolutions to vision transformers.このような場合、"Science "は "Science "を意味し、"Science "は "Science "を意味する。また、このような場合、「李舜臣」 は、「李舜臣」と呼ばれる。このような場合，"Aggregated residual transformations for deep neural networks "と呼ぶ．In CVPR, pages 1492-1500, 2017.[63] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.反復メッセージパッシングによるシーングラフ生成．In CVPR, pages 5410-5419, 2017.[64] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.How powerful are graph neural networks?In ICLR, 2018.[65] Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang.バイナリニューラルネットワークのための周波数領域近似の学習。また、このような場合にも、「李舜臣」 は、「李舜臣」と呼ばれる。クラウド上のPositiveunlabeled圧縮。NeurIPS, volume 32, 2019 に掲載。[67] Sijie Yan, Yuanjun Xiong, and Dahua Lin.Spatial temporal graph convolutional networks for skeletonbased action recognition（骨格に基づく行動認識のための空間的時間グラフ畳み込みネットワーク）。In AAAI, 2018.[68] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh.シーングラフ生成のためのグラフr-cnn.In ECCV, pages 670-685, 2018.12
### Page13
[また、このような場合、「曖昧さ」を解消するために、「曖昧さ」を解消するために、「曖昧さ」を解消するために、「曖昧さ」を解消するために、「曖昧さ」を解消するために、「曖昧さ」を解消する必要がある。このような場合、"industilling knowledge from graphconvolutional networks "と呼ぶ。また、このような俯瞰的な視点に立脚することで、より高度な知見を得ることができる。Cars:連続的な進化による効率的なニューラルアーキテクチャの探索。また、このような "萌え "を実現するために、"萌え "であることが重要である。Metaformer は実はビジョンに必要なものである。このような場合、"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.Cutmix:局所化可能な特徴を持つ強力なクラシファを訓練するための正則化戦略。In ICCV, 2019.[73] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup:経験的なリスクミニマイゼーションを超えて。ICLR, 2018 に掲載。[74] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.Random erasing data augmentation.In AAAI, volume 34, pages 13001-13008, 2020.[75] Barret Zoph and Quoc V Le.（バレット・ゾフ、クオック・ヴィー・レ）。強化学習によるニューラル・アーキテクチャ探索.In ICLR, 2017.Checklist1.全著者対象...(a) アブストラクトとイントロダクションで述べている主な主張は、論文の貢献と範囲を正確に反映しているか？[Yes] セクション 1 参照(b) 自分の研究の限界について記述したか？[はい】(c) 研究の社会的な潜在的なマイナス影響について説明しましたか？[No] 潜在的な社会的影響はない。 (d) 倫理審査ガイドラインを読み、論文がそれに適合していることを確認しましたか。[はい】2．(a)理論的な結果について、その前提条件をすべて述べましたか？[該当なし](b)すべての理論的な結果について、完全な証明を含んでいましたか？[該当なし]3.実験を行った場合...(a) 主要な実験結果を再現するために必要なコード、データ、説明書を添付したか（補足資料またはURLとして）？[b）学習の詳細（データ分割、ハイパーパラメータ、それらの選択方法など）をすべて明記しましたか。[Yes] セクション 4.1 参照(c) エラーバー（例えば、複数回実験を行った際のランダムシードに関するもの）を報告しましたか？[いいえ】ImageNet と COCO データセットで共通の設定です。 d）総計算量と使用したリソースの種類（例：GPU の種類、内部クラスタ、クラウドプロバイダ）を記載しましたか？[はい】セクション 4.1.4 を参照してください。既存の資産(コード、データ、モデルなど)を使用している場合、または新しい資産を管理・公開している場合...(a) 作品が既存の資産を使用している場合、その作成者を引用しましたか？[はい] セクション4.1参照(b) アセットのライセンスについて言及しましたか？[はい] セクション4.1参照(c) 新しいアセットを補足資料またはURLとして掲載しましたか？[いいえ](d) データを利用/収集する人から同意を得たかどうか、またその方法について説明しましたか。[該当なし](e)利用/収集するデータに個人を特定できる情報や不快なコンテンツが含まれているかどうかを議論しましたか？[該当なし]5.クラウドソーシングを利用した場合、または人間を対象とした研究を行った場合...(a) 参加者に与えた指示の全文と、該当する場合はスクリーンショットを掲載しましたか？[該当なし](b) 参加者の潜在的なリスクについて説明し、該当する場合は、施設審査委員会（IRB）の承認へのリンクがありましたか？[N/A](c) 参加者に支払われる推定時給と、参加者の報酬に費やされた総額を記載しましたか？[該当なし]13
### Page14
A付録A.1理論解析ViGブロックでは，FFNモジュールなどの特徴変換をより多く利用することにより，ノードの特徴多様性を高めることを提案する．本論文では、Vanilla ResGCNと我々のViGモデルの実証的な比較を示す。ここでは、ViGにおけるFFNモジュールが特徴の多様性を高める上でどのような効果があるのかについて、簡単な理論解析を行う。グラフ畳み込みの出力特徴量X∈RN×Dが与えられたとき、特徴多様性[8]はγ(X) = ∥X - 1xT ∥,wherex = arg minx ∥X - 1xT ∥,(9) ∥ - ∥は行列のℓ1，∞ノルムとして測られます。この特徴量に対してFFNモジュールを適用することにより，以下の定理が成り立つ．FFNモジュールが与えられたとき，その出力特徴量の多様性γ(FFN(X))はγ(FFN(X)) ≦ λγ(X),(10)where λ is the Lipschitz constant of FFN regarding p-norm for p∈ [1, ∞].Proof.FNNモジュールの出力特徴量の多様性は，(1) ≦ (10) を満足する.FFNは重み行列の乗算、バイアス加算、要素ごとの非線形関数を含み、これらはすべてFFN(1xT )のconstancy-across-rows特性を保存する。したがって、γ(FFN(X))= ∥FFN(X) - 1x′T ∥p≤ ∥FFN(X) - FFN(1xT )∥p▷ FFNはconstancy-across-rowsを保存していることがわかります。≦λ∥X - 1xT ∥p▷ リプシッツ定数の定義.= λγ(X),FFNのリプシッツ定数は重み行列のノルムに関係し、通常1よりはるかに大きい[54]。A.2 擬似コード提案するVision GNNフレームワークは、複雑な演算を導入することなく、一般的に使用されている層に基づいて容易に実装することが可能である。コア部分であるViGブロックの擬似コードをAlgorithmに示す．Algorithm 1 PyTorch-like Code of ViG Blockimport torch.nn as nnfrom gcn_lib.dense.torch_vertex import DynConv2d# gcn_lib is downloaded from https://github.com/lightaime/deep_gcns_torchclass GrapherModule(nn.Nn.Nn):"""GrapherModule(nn.Nn.Nn):"""GrapherModule(nn.Nn.Module):"""Grapher module with graph conv and FC layers"""def __init__(self, in_channels, hidden_channels, k=9, dilation=1, drop_path=0.0):super(GrapherModule, self).__init__()self.fc1 = nn.Conversion(nn.Conversion, FC).Grapher.Conversion(self).Grapher.Conversion(self).GrapherModule(self).GrapherModule(self)Sequential(nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),nn.BatchNorm2d(in_channels),)self.graph_conv = nn.Sequential(DynConv2d(in_channels, hidden_channels, k, dilation, act=None),nn.BatchNorm2d(hidden_channels),nn.BatchNorm1d(in_channels, 1),nn.Conv2d(nn.BatchNorm1d, 1),)self.graph_conv = nn.Conv2d(in_channels, in_channels),)self.gelu(self, hidden_channels)GELU(),)self.fc2 = nn.Sequential(nn.Conv2d(hidden_channels, in_channels, 1, stride=1, padding=0),nn.BatchNorm2d(in_channels),)self.drop_path = DropPath(drop_path) if drop_path > 0.else nn.Identity()14
### Page15
def forward(self, x):B, C, H, W = x.shapex = x.reshape(B, C, -1, 1).contiguous()shortcut = xx = self.fc1(x)x = self.graph_conv(x)x = self.fc2(x)x = self.drop_path(x) + shortcutreturn x.reshape(B, C, H, W)class FFNModule(nn.Module):"""Feed-forward Network"""def __init__(self, in_channels, hidden_channels, drop_path=0.0.0).BatchNorm2d(hidden_channels),nn.GELU())self.fc2 = nn.Sequential(nn.Conv2d(hidden_channels, in_channels, 1, stride=1, padding=0),nn.Fc2 = nn.Sequential(nn.Conv2d(hidden_channels, 1), stride=1, padding=0), nn.GELU()))BatchNorm2d(in_channels),)self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()def forward(self, x):shortcut = xx = self.fc1(x)x = self.fc2(x)x = self.drop_path(x) + shortcutreturn xclass ViGBlock(nn.Module):"""ViG block with Grapher and FFN modules"""def __init__(self, channels, k, dilation, drop_path=0.0):super(ViGBlock, self).__init__()self.grapher = GrapherModule(channels, channels * 2, k, dilation, drop_path)self.ffn = FFNModule(channels, channels * 4, drop_path)def forward(self, x):x = self.grapher(x)x = self.ffn(x)return x15

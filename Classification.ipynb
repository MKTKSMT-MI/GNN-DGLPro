{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kisim\\anaconda3\\envs\\GNN_DGL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.data\n",
    "from dgl.nn import GraphConv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トレーニング用データセットクラス\n",
    "class CIFAR10TrainDataset(DGLDataset):\n",
    "    def __init__(self,data_path):\n",
    "        self.data_path = data_path\n",
    "        super().__init__(name='cifar10_train__gprah')\n",
    "    \n",
    "    def process(self):\n",
    "        GRAPHS, LABELS = dgl.load_graphs(self.data_path) #保存したグラーフデータの読み込み\n",
    "        self.graphs = GRAPHS #グラフリストを代入\n",
    "        self.labels = LABELS['label'] #ラベル辞書の値のみ代入\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "\n",
    "class CIFAR10TestDataset(DGLDataset):\n",
    "    def __init__(self,data_path):\n",
    "        self.data_path = data_path\n",
    "        super().__init__(name='cifar10_test_gprah')\n",
    "    \n",
    "    def process(self):\n",
    "        GRAPHS, LABELS = dgl.load_graphs(self.data_path) #保存したグラーフデータの読み込み\n",
    "        self.graphs = GRAPHS #グラフリストを代入\n",
    "        self.labels = LABELS['label'] #ラベル辞書の値のみ代入\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/MyDataset/train_graphs_six_f_in_pos_50_std.dgl\"\n",
    "traindataset = CIFAR10TrainDataset(\"./data/NewMyData/train_6feat_dist_40.dgl\")\n",
    "testdataset = CIFAR10TestDataset(\"./data/NewMyData/test_6feat_dist_40.dgl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_wokers = 0\n",
      "nt\n"
     ]
    }
   ],
   "source": [
    "if os.name =='posix':\n",
    "    num_workers = 2\n",
    "else:\n",
    "    num_workers = 0\n",
    "#num_workers = 0\n",
    "traindataloader = GraphDataLoader(traindataset,batch_size = 2500,shuffle = True,num_workers = num_workers,pin_memory = True)\n",
    "testdataloader = GraphDataLoader(testdataset,batch_size = 5000,shuffle = True,num_workers = num_workers,pin_memory = True)\n",
    "print(f'num_wokers = {num_workers}')\n",
    "print(os.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n",
      "tensor([ 0.0000,  3.6056,  6.7082, 10.0499,  9.4868,  8.2462,  9.2195,  1.4142,\n",
      "         5.8310,  4.0000, 10.1980,  4.1231,  7.6158,  8.5440,  5.0000,  3.0000,\n",
      "         1.0000,  3.1623,  2.2361,  2.0000])\n",
      "tensor([3.6056, 0.0000, 4.0000, 7.6158, 7.8102, 6.4031, 7.2111, 2.2361, 3.0000,\n",
      "        2.2361, 8.0623, 1.4142, 6.4031, 7.0711, 2.0000, 2.0000, 2.8284, 1.0000,\n",
      "        1.4142, 2.2361])\n",
      "tensor([ 6.7082,  4.0000,  0.0000,  9.8995, 10.8167,  9.4340, 10.0000,  5.3852,\n",
      "         1.0000,  6.0828, 10.6301,  5.0990,  9.8489, 10.2956,  2.0000,  6.0000,\n",
      "         6.3246,  5.0000,  5.0990,  6.0828])\n",
      "tensor([10.0499,  7.6158,  9.8995,  0.0000,  2.2361,  2.2361,  1.4142,  9.2195,\n",
      "         9.2195,  6.0828,  1.0000,  6.3246,  3.6056,  2.8284,  8.6023,  7.0711,\n",
      "         9.0554,  7.2801,  8.2462,  8.0623])\n",
      "tensor([ 9.4868,  7.8102, 10.8167,  2.2361,  0.0000,  1.4142,  1.0000,  8.9443,\n",
      "        10.0000,  5.8310,  1.4142,  6.4031,  2.0000,  1.0000,  9.2195,  6.7082,\n",
      "         8.5440,  7.2111,  8.0623,  7.6158])\n",
      "tensor([[ 0.0000,  3.1623,  4.4721,  4.2426, 19.3132,  1.4142, 10.4403, 19.6977,\n",
      "          2.2361,  1.4142,  3.1623,  3.6056,  1.0000,  2.8284,  1.4142,  4.4721,\n",
      "          3.6056,  1.0000,  2.0000,  2.2361],\n",
      "        [ 3.1623,  0.0000,  7.6158,  2.0000, 21.8403,  4.4721, 13.6015, 22.1359,\n",
      "          1.0000,  2.0000,  6.3246,  1.0000,  3.0000,  5.8310,  4.0000,  1.4142,\n",
      "          6.7082,  4.1231,  5.0990,  5.3852],\n",
      "        [ 4.4721,  7.6158,  0.0000,  8.6023, 16.6433,  3.1623,  6.0828, 17.2047,\n",
      "          6.7082,  5.8310,  1.4142,  8.0623,  5.0000,  2.0000,  4.2426,  8.9443,\n",
      "          1.0000,  3.6056,  2.8284,  2.2361],\n",
      "        [ 4.2426,  2.0000,  8.6023,  0.0000, 21.3776,  5.6569, 14.3178, 21.5870,\n",
      "          2.2361,  2.8284,  7.2111,  1.0000,  3.6056,  7.0711,  4.4721,  1.4142,\n",
      "          7.8102,  5.0000,  5.8310,  6.4031],\n",
      "        [19.3132, 21.8403, 16.6433, 21.3776,  0.0000, 18.7883, 12.8062,  1.0000,\n",
      "         20.8806, 19.9249, 17.0000, 21.5870, 18.9737, 18.3576, 18.0278, 22.5610,\n",
      "         17.4929, 18.3848, 17.4642, 17.8885],\n",
      "        [ 1.4142,  4.4721,  3.1623,  5.6569, 18.7883,  0.0000,  9.2195, 19.2354,\n",
      "          3.6056,  2.8284,  2.0000,  5.0000,  2.2361,  1.4142,  2.0000,  5.8310,\n",
      "          2.2361,  1.0000,  1.4142,  1.0000],\n",
      "        [10.4403, 13.6015,  6.0828, 14.3178, 12.8062,  9.2195,  0.0000, 13.6015,\n",
      "         12.6491, 11.7047,  7.2801, 13.9284, 10.7703,  8.0623,  9.8489, 14.8661,\n",
      "          7.0711,  9.4868,  8.5440,  8.2462],\n",
      "        [19.6977, 22.1359, 17.2047, 21.5870,  1.0000, 19.2354, 13.6015,  0.0000,\n",
      "         21.1896, 20.2485, 17.4929, 21.8403, 19.3132, 18.8680, 18.3848, 22.8035,\n",
      "         18.0278, 18.7883, 17.8885, 18.3576],\n",
      "        [ 2.2361,  1.0000,  6.7082,  2.2361, 20.8806,  3.6056, 12.6491, 21.1896,\n",
      "          0.0000,  1.0000,  5.3852,  1.4142,  2.0000,  5.0000,  3.0000,  2.2361,\n",
      "          5.8310,  3.1623,  4.1231,  4.4721],\n",
      "        [ 1.4142,  2.0000,  5.8310,  2.8284, 19.9249,  2.8284, 11.7047, 20.2485,\n",
      "          1.0000,  0.0000,  4.4721,  2.2361,  1.0000,  4.2426,  2.0000,  3.1623,\n",
      "          5.0000,  2.2361,  3.1623,  3.6056],\n",
      "        [ 3.1623,  6.3246,  1.4142,  7.2111, 17.0000,  2.0000,  7.2801, 17.4929,\n",
      "          5.3852,  4.4721,  0.0000,  6.7082,  3.6056,  1.4142,  2.8284,  7.6158,\n",
      "          1.0000,  2.2361,  1.4142,  1.0000],\n",
      "        [ 3.6056,  1.0000,  8.0623,  1.0000, 21.5870,  5.0000, 13.9284, 21.8403,\n",
      "          1.4142,  2.2361,  6.7082,  0.0000,  3.1623,  6.4031,  4.1231,  1.0000,\n",
      "          7.2111,  4.4721,  5.3852,  5.8310],\n",
      "        [ 1.0000,  3.0000,  5.0000,  3.6056, 18.9737,  2.2361, 10.7703, 19.3132,\n",
      "          2.0000,  1.0000,  3.6056,  3.1623,  0.0000,  3.6056,  1.0000,  4.1231,\n",
      "          4.2426,  1.4142,  2.2361,  2.8284],\n",
      "        [ 2.8284,  5.8310,  2.0000,  7.0711, 18.3576,  1.4142,  8.0623, 18.8680,\n",
      "          5.0000,  4.2426,  1.4142,  6.4031,  3.6056,  0.0000,  3.1623,  7.2111,\n",
      "          1.0000,  2.2361,  2.0000,  1.0000],\n",
      "        [ 1.4142,  4.0000,  4.2426,  4.4721, 18.0278,  2.0000,  9.8489, 18.3848,\n",
      "          3.0000,  2.0000,  2.8284,  4.1231,  1.0000,  3.1623,  0.0000,  5.0990,\n",
      "          3.6056,  1.0000,  1.4142,  2.2361],\n",
      "        [ 4.4721,  1.4142,  8.9443,  1.4142, 22.5610,  5.8310, 14.8661, 22.8035,\n",
      "          2.2361,  3.1623,  7.6158,  1.0000,  4.1231,  7.2111,  5.0990,  0.0000,\n",
      "          8.0623,  5.3852,  6.3246,  6.7082],\n",
      "        [ 3.6056,  6.7082,  1.0000,  7.8102, 17.4929,  2.2361,  7.0711, 18.0278,\n",
      "          5.8310,  5.0000,  1.0000,  7.2111,  4.2426,  1.0000,  3.6056,  8.0623,\n",
      "          0.0000,  2.8284,  2.2361,  1.4142],\n",
      "        [ 1.0000,  4.1231,  3.6056,  5.0000, 18.3848,  1.0000,  9.4868, 18.7883,\n",
      "          3.1623,  2.2361,  2.2361,  4.4721,  1.4142,  2.2361,  1.0000,  5.3852,\n",
      "          2.8284,  0.0000,  1.0000,  1.4142],\n",
      "        [ 2.0000,  5.0990,  2.8284,  5.8310, 17.4642,  1.4142,  8.5440, 17.8885,\n",
      "          4.1231,  3.1623,  1.4142,  5.3852,  2.2361,  2.0000,  1.4142,  6.3246,\n",
      "          2.2361,  1.0000,  0.0000,  1.0000],\n",
      "        [ 2.2361,  5.3852,  2.2361,  6.4031, 17.8885,  1.0000,  8.2462, 18.3576,\n",
      "          4.4721,  3.6056,  1.0000,  5.8310,  2.8284,  1.0000,  2.2361,  6.7082,\n",
      "          1.4142,  1.4142,  1.0000,  0.0000]])\n",
      "Dataset(\"cifar10_train__gprah\", num_graphs=20000, save_path=C:\\Users\\kisim\\.dgl\\cifar10_train__gprah)\n"
     ]
    }
   ],
   "source": [
    "print(traindataset[0][0].ndata['feat value'].shape)\n",
    "for i in range(5):\n",
    "    print(traindataset[3][0].ndata['feat value'][i])\n",
    "print(traindataset[9][0].ndata['feat value'])\n",
    "print(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(\"cifar10_train__gprah\", num_graphs=20000, save_path=C:\\Users\\kisim\\.dgl\\cifar10_train__gprah)\n",
      "torch.Size([5, 5])\n",
      "tensor([0.0000, 2.0000, 1.0000, 1.4142, 1.0000])\n",
      "tensor([2.0000, 0.0000, 2.2361, 1.4142, 1.0000])\n",
      "tensor([1.0000, 2.2361, 0.0000, 1.0000, 1.4142])\n",
      "tensor([1.4142, 1.4142, 1.0000, 0.0000, 1.0000])\n",
      "tensor([1.0000, 1.0000, 1.4142, 1.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "print(traindataset)\n",
    "print(traindataset[0][0].ndata['feat value'].shape)\n",
    "for i in range(5):\n",
    "    print(traindataset[3][0].ndata['feat value'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク設定\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN,self).__init__()\n",
    "        self.conv1 = GraphConv(6,16)\n",
    "        self.conv2 = GraphConv(16,32)\n",
    "        self.conv3 = GraphConv(32,128)\n",
    "        self.conv4 = GraphConv(64,128)\n",
    "        self.dropout =nn.Dropout(0.4)\n",
    "        self.meanpooling = nn.AvgPool1d(2)\n",
    "        self.maxpooling = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,10)\n",
    "\n",
    "\n",
    "    def forward(self,g,n_feat,e_feat = None):\n",
    "        h = self.conv1(g,n_feat,None,e_feat)\n",
    "        h = self.conv2(g,h,None,e_feat)\n",
    "        #h = self.dropout(h)\n",
    "        h = self.conv3(g,h,None,e_feat)\n",
    "\n",
    "        #h = self.meanpooling(h)\n",
    "        h = self.maxpooling(h)\n",
    "\n",
    "        h = self.flatten(h)\n",
    "\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = self.fc4(h)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        g.ndata['h'] = h\n",
    "\n",
    "        return dgl.mean_nodes(g,'h')\n",
    "        #return dgl.softmax_nodes(g,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 32 64 128 x4 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.02)\n",
    "#optimizer = optim.SGD(params=model.parameters(),lr=0.03,momentum=0.9)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_acc = []\n",
    "save_test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:36<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.33955\n",
      "Test accuracy: 0.3196\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "test_num_correct = 0\n",
    "test_num_tests = 0\n",
    "#,batched_graph.edata['distance'].float()\n",
    "BP = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    if BP != 0:\n",
    "        break\n",
    "    model.train()\n",
    "    for batched_graph, labels in traindataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "        loss = F.cross_entropy(pred,labels)\n",
    "        if loss.item() < 0.05:\n",
    "            BP = 0\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    loss_list.append(loss.item())\n",
    "    acc_list.append(num_correct / num_tests)\n",
    "    \n",
    "    model.eval()\n",
    "    for tbatched_graph, tlabels in testdataloader:\n",
    "        tbatched_graph = tbatched_graph.to(device)\n",
    "        tlabels = tlabels.to(device)\n",
    "        tpred = model(tbatched_graph, tbatched_graph.ndata['feat value'])\n",
    "        test_num_correct += (tpred.argmax(1) == tlabels).sum().item()\n",
    "        test_num_tests += len(tlabels)\n",
    "\n",
    "    Tacc = test_num_correct / test_num_tests\n",
    "    #print('Training accuracy:', Tacc)\n",
    "    #test_acc_list.append(Tacc)\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.train()\n",
    "    for batched_graph, labels in traindataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(batched_graph, batched_graph.ndata['feat value'])\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "\n",
    "    print('Training accuracy:', num_correct / num_tests)\n",
    "    save_train_acc.append(num_correct / num_tests)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for batched_graph, labels in testdataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "\n",
    "    print('Test accuracy:', num_correct / num_tests)\n",
    "    save_test_acc.append(num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12500, 10])\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "tensor(1.7852, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#5:40\n",
    "#3:15\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [p for p in range(1,len(loss_list)+1)]\n",
    "y = loss_list\n",
    "plt.plot(x,y)\n",
    "plt.title('Train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [p for p in range(1,len(loss_list)+1)]\n",
    "y1 = acc_list\n",
    "y2 = test_acc_list\n",
    "plt.ylim(0,1)\n",
    "'''\n",
    "#20\n",
    "plt.plot(x,train_acc_lists[0],label = '20 Train acc', color = \"crimson\")\n",
    "plt.plot(x,test_acc_lists[0],label = '20 Test acc', color = \"crimson\",linestyle = 'dashed')\n",
    "\n",
    "#30\n",
    "plt.plot(x,train_acc_lists[1],label = '30 Train acc', color = \"darkblue\")\n",
    "plt.plot(x,test_acc_lists[1],label = '30 Test acc', color = \"darkblue\",linestyle = 'dashed')\n",
    "\n",
    "#40\n",
    "plt.plot(x,train_acc_lists[2],label = '40 Train acc', color = \"green\")\n",
    "plt.plot(x,test_acc_lists[2],label = '40 Test acc', color = \"green\",linestyle = 'dashed')\n",
    "'''\n",
    "plt.plot(x,y1,label='train acc')\n",
    "plt.plot(x,y2,label='test acc')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Training and Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4065, 0.4174, 0.4706, 0.412, 0.4171, 0.41573333333333334, 0.41586666666666666, 0.4169333333333333, 0.4116666666666667, 0.41386666666666666, 0.4119, 0.4167, 0.4145, 0.4161666666666667, 0.4157666666666667, 0.4141]\n",
      "[0.3377, 0.3246, 0.2941, 0.3054, 0.2995, 0.298, 0.3004, 0.3012, 0.2941, 0.2941, 0.3006, 0.2993, 0.3011, 0.2949, 0.3034, 0.2941]\n"
     ]
    }
   ],
   "source": [
    "print(save_train_acc)\n",
    "print(save_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x*x-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_x_value = 5\n",
    "ini_x_tensor = ini_x_value*torch.ones(1, 1, dtype = torch.float64)\n",
    "x = Variable(ini_x_tensor, requires_grad=True)\n",
    "print(f'roop {0:<4d} x = {x.item()}')\n",
    "\n",
    "roop = 0\n",
    "while roop < 10:\n",
    "    roop += 1\n",
    "    # 勾配の計算\n",
    "    f(x).backward()\n",
    "    # xの更新\n",
    "    x.data -= (f(x)/x.grad).data\n",
    "    # 勾配を0に設定\n",
    "    x.grad.zero_()\n",
    "    print(f'roop {roop:<4d} x = {x.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なもの\n",
    "lossの履歴  \n",
    "trainのacc  \n",
    "testのacc  \n",
    "実行時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logs = []\n",
    "train_acc_logs = []\n",
    "test_acc_logs = []\n",
    "run_time_logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-16-32-64-128-128-64-32-16-10\n",
    "#8-64-512-512-64-10\n",
    "#8-96-1152-1152-500-150-20-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク設定\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN1,self).__init__()\n",
    "        self.conv1 = GraphConv(8,64)\n",
    "        self.conv2 = GraphConv(64,512)\n",
    "    \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.liner1 = torch.nn.Linear(512,64)\n",
    "        self.liner2 = torch.nn.Linear(64,10)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p = 0.2)\n",
    "\n",
    "    def forward(self,g,n_feat,e_feat = None):\n",
    "        h = F.relu(self.conv1(g,n_feat,None,e_feat))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(g,h,None,e_feat)\n",
    "        \n",
    "        h = self.flatten(h)\n",
    "\n",
    "        h = F.relu(self.liner1(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.liner2(h)\n",
    "        \n",
    "        g.ndata['h'] = h\n",
    "\n",
    "        return dgl.mean_nodes(g,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク設定\n",
    "class GCN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN2,self).__init__()\n",
    "        self.conv1 = GraphConv(8,96)\n",
    "        self.conv2 = GraphConv(96,1152)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.liner1 = torch.nn.Linear(1152,500)\n",
    "        self.liner2 = torch.nn.Linear(500,150)\n",
    "        self.liner3 = torch.nn.Linear(150,20)\n",
    "        self.liner4 = torch.nn.Linear(20,10)\n",
    "        self.dropout = torch.nn.Dropout(p = 0.2)\n",
    "\n",
    "    def forward(self,g,n_feat,e_feat = None):\n",
    "        h = F.relu(self.conv1(g,n_feat,None,e_feat))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(g,h,None,e_feat)\n",
    "        \n",
    "        h = self.flatten(h)\n",
    "\n",
    "        h = F.relu(self.liner1(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.liner2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.liner3(h)\n",
    "        \n",
    "        g.ndata['h'] = h\n",
    "\n",
    "        return dgl.mean_nodes(g,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_list = []\n",
    "opt_list = []\n",
    "model1 = GCN1()\n",
    "model_list.append(model1)\n",
    "model2 = GCN2()\n",
    "model_list.append(model2)\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "optimizer1 = optim.Adam(model1.parameters(),lr = 0.01)\n",
    "opt_list.append(optimizer1)\n",
    "optimizer2 = optim.Adam(model2.parameters(),lr = 0.01)\n",
    "opt_list.append(optimizer2)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_list)):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    start = time.time()\n",
    "    runmodel = model_list[i]\n",
    "    opt = opt_list[i]\n",
    "    runmodel.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batched_graph,labels in traindataloader:\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = runmodel(batched_graph, batched_graph.ndata['feat value'].float(),batched_graph.edata['distance'].float())\n",
    "            loss = F.cross_entropy(pred,labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "            num_tests += len(labels)\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(num_correct / num_tests)\n",
    "    loss_logs.append(loss_list)\n",
    "    run_time_logs.append(time.time() - start)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    runmodel.eval()\n",
    "    for batched_graph,labels in traindataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = runmodel(batched_graph, batched_graph.ndata['feat value'].float(),batched_graph.edata['distance'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    train_acc_logs.append(num_correct / num_tests)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    for batched_graph,labels in testdataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = runmodel(batched_graph, batched_graph.ndata['feat value'].float(),batched_graph.edata['distance'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    test_acc_logs.append(num_correct / num_tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_logs)\n",
    "print(train_acc_logs)\n",
    "print(test_acc_logs)\n",
    "print(run_time_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlog = model_list[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlog.__sizeof__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
    "#optimizer = optim.SGD(params=model.parameters(),lr=0.001,momentum=0.9)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
    "#optimizer = optim.SGD(params=model.parameters(),lr=0.001,momentum=0.9)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6])\n",
    "x_label = ['5','10','20','30','40','50']\n",
    "normal_Train_acc = [0.4089,0.3904,0.4532,0.4585,0.459,0.]\n",
    "normal_Test_acc = [0.2325,0.281,0.285,0.3005,0.302,0.]\n",
    "incpos_Train_acc = [0.4575,0.48,0.4853,0.4418,0.4774,0.]\n",
    "incpos_Test_acc = [0.2375,0.294,0.3035,0.34,0.3345,0.]\n",
    "std_Train_acc = [0.5268,0.5211,0.5501,0.5672,0.5776,0.5802]\n",
    "std_Test_acc = [0.2125,0.244,0.313,0.3255,0.364,0.3715]\n",
    "Train_data = [normal_Train_acc,incpos_Train_acc,std_Train_acc]\n",
    "Test_data = [normal_Test_acc,incpos_Test_acc,std_Test_acc]\n",
    "margin = 0.2\n",
    "totoal_width = 1 - margin\n",
    "\n",
    "for i,h in enumerate(Train_data):\n",
    "    pos = x - totoal_width *( 1- (2*i+1)/len(Train_data) )/2\n",
    "    plt.bar(pos, h, width = totoal_width/len(Train_data))\n",
    "plt.xticks(x,x_label)\n",
    "plt.title('Training acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(Train_data):\n",
    "    pos = x - totoal_width *( 1- (2*i+1)/len(Test_data) )/2\n",
    "    plt.bar(pos, h, width = totoal_width/len(Test_data))\n",
    "plt.xticks(x,x_label)\n",
    "plt.title('Test acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmodel = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torchmodel.to(device)\n",
    "optimizer = optim.Adam(torchmodel.parameters(),lr = 0.001)\n",
    "epochs = 15\n",
    "\n",
    "history = {'train_loss':[],'train_acc':[],'test_acc':[]}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for e in range(epochs):\n",
    "    torchmodel.train()\n",
    "    loss = None\n",
    "\n",
    "    for i,(batched_graph, labels) in enumerate(traindataloader):\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = torchmodel(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "        loss = F.cross_entropy(pred,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if (i+1) % 10 == 0:\n",
    "        #    print(f'Training log: {e+1} epoch ({(i+1)*200} / 10000 train. data). Loss: {loss.item()}')\n",
    "\n",
    "    history['train_loss'].append(loss.item())\n",
    "\n",
    "    torchmodel.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(batched_graph, labels) in enumerate(tqdm(traindataloader)):\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = torchmodel(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "            correct += (pred.argmax(1) == labels).sum().item()\n",
    "            num_tests += len(labels)\n",
    "\n",
    "    acc = float(correct/num_tests)\n",
    "    history['train_acc'].append(acc)\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(batched_graph, labels) in enumerate(tqdm(testdataloader)):\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = torchmodel(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "            correct += (pred.argmax(1) == labels).sum().item()\n",
    "            num_tests += len(labels)\n",
    "\n",
    "    acc = float(correct/num_tests)\n",
    "    history['test_acc'].append(acc)\n",
    "\n",
    "max_train_acc = max(history['train_acc'])\n",
    "min_train_loss = min(history['train_loss'])\n",
    "max_test_acc = max(history['test_acc'])\n",
    "\n",
    "print(f'Max train accuracy: {max_train_acc}')\n",
    "print(f'Min train loss: {min_train_loss}')\n",
    "print(f'Max test acc: {max_test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('GNN_DGL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e17f8d3ee38eec6defc8bec53d4ec895188fbdc79a3238a84d9b54a33d5e0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

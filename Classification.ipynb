{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.data\n",
    "from dgl.nn import GraphConv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トレーニング用データセットクラス\n",
    "class CIFAR10TrainDataset(DGLDataset):\n",
    "    def __init__(self,data_path):\n",
    "        self.data_path = data_path\n",
    "        super().__init__(name='cifar10_train__gprah')\n",
    "    \n",
    "    def process(self):\n",
    "        GRAPHS, LABELS = dgl.load_graphs(self.data_path) #保存したグラーフデータの読み込み\n",
    "        self.graphs = GRAPHS #グラフリストを代入\n",
    "        self.labels = LABELS['label'] #ラベル辞書の値のみ代入\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "\n",
    "class CIFAR10TestDataset(DGLDataset):\n",
    "    def __init__(self,data_path):\n",
    "        self.data_path = data_path\n",
    "        super().__init__(name='cifar10_test_gprah')\n",
    "    \n",
    "    def process(self):\n",
    "        GRAPHS, LABELS = dgl.load_graphs(self.data_path) #保存したグラーフデータの読み込み\n",
    "        self.graphs = GRAPHS #グラフリストを代入\n",
    "        self.labels = LABELS['label'] #ラベル辞書の値のみ代入\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/MyDataset/train_graphs_six_f_in_pos_50_std.dgl\"\n",
    "traindataset = CIFAR10TrainDataset(\"./data/NewMyData/train_dist_40_std.dgl\")\n",
    "testdataset = CIFAR10TestDataset(\"./data/NewMyData/test_dist_40.dgl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_wokers = 0\n",
      "posix\n"
     ]
    }
   ],
   "source": [
    "if os.name =='posix':\n",
    "    num_workers = 2\n",
    "else:\n",
    "    num_workers = 0\n",
    "num_workers = 0\n",
    "traindataloader = GraphDataLoader(traindataset,batch_size = 2500,shuffle = True,num_workers = num_workers,pin_memory = True)\n",
    "testdataloader = GraphDataLoader(testdataset,batch_size = 5000,shuffle = True,num_workers = num_workers,pin_memory = True)\n",
    "print(f'num_wokers = {num_workers}')\n",
    "print(os.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n",
      "tensor([ 0.0000,  3.6056,  6.7082, 10.0499,  9.4868,  8.2462,  9.2195,  1.4142,\n",
      "         5.8310,  4.0000, 10.1980,  4.1231,  7.6158,  8.5440,  5.0000,  3.0000,\n",
      "         1.0000,  3.1623,  2.2361,  2.0000])\n",
      "tensor([3.6056, 0.0000, 4.0000, 7.6158, 7.8102, 6.4031, 7.2111, 2.2361, 3.0000,\n",
      "        2.2361, 8.0623, 1.4142, 6.4031, 7.0711, 2.0000, 2.0000, 2.8284, 1.0000,\n",
      "        1.4142, 2.2361])\n",
      "tensor([ 6.7082,  4.0000,  0.0000,  9.8995, 10.8167,  9.4340, 10.0000,  5.3852,\n",
      "         1.0000,  6.0828, 10.6301,  5.0990,  9.8489, 10.2956,  2.0000,  6.0000,\n",
      "         6.3246,  5.0000,  5.0990,  6.0828])\n",
      "tensor([10.0499,  7.6158,  9.8995,  0.0000,  2.2361,  2.2361,  1.4142,  9.2195,\n",
      "         9.2195,  6.0828,  1.0000,  6.3246,  3.6056,  2.8284,  8.6023,  7.0711,\n",
      "         9.0554,  7.2801,  8.2462,  8.0623])\n",
      "tensor([ 9.4868,  7.8102, 10.8167,  2.2361,  0.0000,  1.4142,  1.0000,  8.9443,\n",
      "        10.0000,  5.8310,  1.4142,  6.4031,  2.0000,  1.0000,  9.2195,  6.7082,\n",
      "         8.5440,  7.2111,  8.0623,  7.6158])\n",
      "tensor([[ 0.0000,  3.1623,  4.4721,  4.2426, 19.3132,  1.4142, 10.4403, 19.6977,\n",
      "          2.2361,  1.4142,  3.1623,  3.6056,  1.0000,  2.8284,  1.4142,  4.4721,\n",
      "          3.6056,  1.0000,  2.0000,  2.2361],\n",
      "        [ 3.1623,  0.0000,  7.6158,  2.0000, 21.8403,  4.4721, 13.6015, 22.1359,\n",
      "          1.0000,  2.0000,  6.3246,  1.0000,  3.0000,  5.8310,  4.0000,  1.4142,\n",
      "          6.7082,  4.1231,  5.0990,  5.3852],\n",
      "        [ 4.4721,  7.6158,  0.0000,  8.6023, 16.6433,  3.1623,  6.0828, 17.2047,\n",
      "          6.7082,  5.8310,  1.4142,  8.0623,  5.0000,  2.0000,  4.2426,  8.9443,\n",
      "          1.0000,  3.6056,  2.8284,  2.2361],\n",
      "        [ 4.2426,  2.0000,  8.6023,  0.0000, 21.3776,  5.6569, 14.3178, 21.5870,\n",
      "          2.2361,  2.8284,  7.2111,  1.0000,  3.6056,  7.0711,  4.4721,  1.4142,\n",
      "          7.8102,  5.0000,  5.8310,  6.4031],\n",
      "        [19.3132, 21.8403, 16.6433, 21.3776,  0.0000, 18.7883, 12.8062,  1.0000,\n",
      "         20.8806, 19.9249, 17.0000, 21.5870, 18.9737, 18.3576, 18.0278, 22.5610,\n",
      "         17.4929, 18.3848, 17.4642, 17.8885],\n",
      "        [ 1.4142,  4.4721,  3.1623,  5.6569, 18.7883,  0.0000,  9.2195, 19.2354,\n",
      "          3.6056,  2.8284,  2.0000,  5.0000,  2.2361,  1.4142,  2.0000,  5.8310,\n",
      "          2.2361,  1.0000,  1.4142,  1.0000],\n",
      "        [10.4403, 13.6015,  6.0828, 14.3178, 12.8062,  9.2195,  0.0000, 13.6015,\n",
      "         12.6491, 11.7047,  7.2801, 13.9284, 10.7703,  8.0623,  9.8489, 14.8661,\n",
      "          7.0711,  9.4868,  8.5440,  8.2462],\n",
      "        [19.6977, 22.1359, 17.2047, 21.5870,  1.0000, 19.2354, 13.6015,  0.0000,\n",
      "         21.1896, 20.2485, 17.4929, 21.8403, 19.3132, 18.8680, 18.3848, 22.8035,\n",
      "         18.0278, 18.7883, 17.8885, 18.3576],\n",
      "        [ 2.2361,  1.0000,  6.7082,  2.2361, 20.8806,  3.6056, 12.6491, 21.1896,\n",
      "          0.0000,  1.0000,  5.3852,  1.4142,  2.0000,  5.0000,  3.0000,  2.2361,\n",
      "          5.8310,  3.1623,  4.1231,  4.4721],\n",
      "        [ 1.4142,  2.0000,  5.8310,  2.8284, 19.9249,  2.8284, 11.7047, 20.2485,\n",
      "          1.0000,  0.0000,  4.4721,  2.2361,  1.0000,  4.2426,  2.0000,  3.1623,\n",
      "          5.0000,  2.2361,  3.1623,  3.6056],\n",
      "        [ 3.1623,  6.3246,  1.4142,  7.2111, 17.0000,  2.0000,  7.2801, 17.4929,\n",
      "          5.3852,  4.4721,  0.0000,  6.7082,  3.6056,  1.4142,  2.8284,  7.6158,\n",
      "          1.0000,  2.2361,  1.4142,  1.0000],\n",
      "        [ 3.6056,  1.0000,  8.0623,  1.0000, 21.5870,  5.0000, 13.9284, 21.8403,\n",
      "          1.4142,  2.2361,  6.7082,  0.0000,  3.1623,  6.4031,  4.1231,  1.0000,\n",
      "          7.2111,  4.4721,  5.3852,  5.8310],\n",
      "        [ 1.0000,  3.0000,  5.0000,  3.6056, 18.9737,  2.2361, 10.7703, 19.3132,\n",
      "          2.0000,  1.0000,  3.6056,  3.1623,  0.0000,  3.6056,  1.0000,  4.1231,\n",
      "          4.2426,  1.4142,  2.2361,  2.8284],\n",
      "        [ 2.8284,  5.8310,  2.0000,  7.0711, 18.3576,  1.4142,  8.0623, 18.8680,\n",
      "          5.0000,  4.2426,  1.4142,  6.4031,  3.6056,  0.0000,  3.1623,  7.2111,\n",
      "          1.0000,  2.2361,  2.0000,  1.0000],\n",
      "        [ 1.4142,  4.0000,  4.2426,  4.4721, 18.0278,  2.0000,  9.8489, 18.3848,\n",
      "          3.0000,  2.0000,  2.8284,  4.1231,  1.0000,  3.1623,  0.0000,  5.0990,\n",
      "          3.6056,  1.0000,  1.4142,  2.2361],\n",
      "        [ 4.4721,  1.4142,  8.9443,  1.4142, 22.5610,  5.8310, 14.8661, 22.8035,\n",
      "          2.2361,  3.1623,  7.6158,  1.0000,  4.1231,  7.2111,  5.0990,  0.0000,\n",
      "          8.0623,  5.3852,  6.3246,  6.7082],\n",
      "        [ 3.6056,  6.7082,  1.0000,  7.8102, 17.4929,  2.2361,  7.0711, 18.0278,\n",
      "          5.8310,  5.0000,  1.0000,  7.2111,  4.2426,  1.0000,  3.6056,  8.0623,\n",
      "          0.0000,  2.8284,  2.2361,  1.4142],\n",
      "        [ 1.0000,  4.1231,  3.6056,  5.0000, 18.3848,  1.0000,  9.4868, 18.7883,\n",
      "          3.1623,  2.2361,  2.2361,  4.4721,  1.4142,  2.2361,  1.0000,  5.3852,\n",
      "          2.8284,  0.0000,  1.0000,  1.4142],\n",
      "        [ 2.0000,  5.0990,  2.8284,  5.8310, 17.4642,  1.4142,  8.5440, 17.8885,\n",
      "          4.1231,  3.1623,  1.4142,  5.3852,  2.2361,  2.0000,  1.4142,  6.3246,\n",
      "          2.2361,  1.0000,  0.0000,  1.0000],\n",
      "        [ 2.2361,  5.3852,  2.2361,  6.4031, 17.8885,  1.0000,  8.2462, 18.3576,\n",
      "          4.4721,  3.6056,  1.0000,  5.8310,  2.8284,  1.0000,  2.2361,  6.7082,\n",
      "          1.4142,  1.4142,  1.0000,  0.0000]])\n",
      "Dataset(\"cifar10_train__gprah\", num_graphs=20000, save_path=C:\\Users\\kisim\\.dgl\\cifar10_train__gprah)\n"
     ]
    }
   ],
   "source": [
    "print(traindataset[0][0].ndata['feat value'].shape)\n",
    "for i in range(5):\n",
    "    print(traindataset[3][0].ndata['feat value'][i])\n",
    "print(traindataset[9][0].ndata['feat value'])\n",
    "print(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(\"cifar10_train__gprah\", num_graphs=20000, save_path=C:\\Users\\kisim\\.dgl\\cifar10_train__gprah)\n",
      "torch.Size([5, 5])\n",
      "tensor([0.0000, 2.0000, 1.0000, 1.4142, 1.0000])\n",
      "tensor([2.0000, 0.0000, 2.2361, 1.4142, 1.0000])\n",
      "tensor([1.0000, 2.2361, 0.0000, 1.0000, 1.4142])\n",
      "tensor([1.4142, 1.4142, 1.0000, 0.0000, 1.0000])\n",
      "tensor([1.0000, 1.0000, 1.4142, 1.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "print(traindataset)\n",
    "print(traindataset[0][0].ndata['feat value'].shape)\n",
    "for i in range(5):\n",
    "    print(traindataset[3][0].ndata['feat value'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク設定\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN,self).__init__()\n",
    "        self.conv1 = GraphConv(40,16)\n",
    "        self.conv2 = GraphConv(16,32)\n",
    "        self.conv3 = GraphConv(32,128)\n",
    "        self.conv4 = GraphConv(64,128)\n",
    "        self.dropout =nn.Dropout(0.4)\n",
    "        self.meanpooling = nn.AvgPool1d(2)\n",
    "        self.maxpooling = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,10)\n",
    "\n",
    "\n",
    "    def forward(self,g,n_feat,e_feat = None):\n",
    "        h = self.conv1(g,n_feat,None,e_feat)\n",
    "        h = self.conv2(g,h,None,e_feat)\n",
    "        #h = self.dropout(h)\n",
    "        h = self.conv3(g,h,None,e_feat)\n",
    "\n",
    "        #h = self.meanpooling(h)\n",
    "        h = self.maxpooling(h)\n",
    "\n",
    "        h = self.flatten(h)\n",
    "\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = self.fc4(h)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        g.ndata['h'] = h\n",
    "\n",
    "        return dgl.mean_nodes(g,'h')\n",
    "        #return dgl.softmax_nodes(g,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク設定\n",
    "class GCNv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNv2,self).__init__()\n",
    "        self.conv1 = GraphConv(40,80)\n",
    "        self.conv2 = GraphConv(80,120)\n",
    "        self.conv3 = GraphConv(120,256)\n",
    "        self.conv4 = GraphConv(64,128)\n",
    "        self.dropout =nn.Dropout(0.4)\n",
    "        self.meanpooling = nn.AvgPool1d(2)\n",
    "        self.maxpooling = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,10)\n",
    "\n",
    "\n",
    "    def forward(self,g,n_feat,e_feat = None):\n",
    "        h = self.conv1(g,n_feat,None,e_feat)\n",
    "        h = self.conv2(g,h,None,e_feat)\n",
    "        #h = self.dropout(h)\n",
    "        h = self.conv3(g,h,None,e_feat)\n",
    "\n",
    "        #h = self.meanpooling(h)\n",
    "        h = self.maxpooling(h)\n",
    "\n",
    "        h = self.flatten(h)\n",
    "\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = self.fc4(h)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        g.ndata['h'] = h\n",
    "\n",
    "        return dgl.mean_nodes(g,'h')\n",
    "        #return dgl.softmax_nodes(g,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 32 64 128 x4 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.008)\n",
    "#optimizer = optim.SGD(params=model.parameters(),lr=0.03,momentum=0.9)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCNv2()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.004)\n",
    "#optimizer = optim.SGD(params=model.parameters(),lr=0.03,momentum=0.9)\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_acc = []\n",
    "save_test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [6:38:17<00:00,  2.39s/it]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8896\n",
      "Test accuracy: 0.1085\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "test_num_correct = 0\n",
    "test_num_tests = 0\n",
    "#,batched_graph.edata['distance'].float()\n",
    "BP = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    if BP != 0:\n",
    "        break\n",
    "    model.train()\n",
    "    for batched_graph, labels in traindataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "        loss = F.cross_entropy(pred,labels)\n",
    "        if loss.item() < 0.05:\n",
    "            BP = 0\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    loss_list.append(loss.item())\n",
    "    acc_list.append(num_correct / num_tests)\n",
    "    \n",
    "    model.eval()\n",
    "    for tbatched_graph, tlabels in testdataloader:\n",
    "        tbatched_graph = tbatched_graph.to(device)\n",
    "        tlabels = tlabels.to(device)\n",
    "        tpred = model(tbatched_graph, tbatched_graph.ndata['feat value'])\n",
    "        test_num_correct += (tpred.argmax(1) == tlabels).sum().item()\n",
    "        test_num_tests += len(tlabels)\n",
    "\n",
    "    Tacc = test_num_correct / test_num_tests\n",
    "    #print('Training accuracy:', Tacc)\n",
    "    #test_acc_list.append(Tacc)\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.train()\n",
    "    for batched_graph, labels in traindataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(batched_graph, batched_graph.ndata['feat value'])\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "\n",
    "    print('Training accuracy:', num_correct / num_tests)\n",
    "    save_train_acc.append(num_correct / num_tests)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for batched_graph, labels in testdataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "\n",
    "    print('Test accuracy:', num_correct / num_tests)\n",
    "    save_test_acc.append(num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12500, 10])\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#5:40\n",
    "#3:15\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [p for p in range(1,len(loss_list)+1)]\n",
    "y = loss_list\n",
    "plt.plot(x,y)\n",
    "plt.title('Train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '10000epoch.pth'\n",
    "torch.save(model.state_dict(),save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file='10000loss_list',arr=loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [p for p in range(1,len(loss_list)+1)]\n",
    "y1 = acc_list\n",
    "y2 = test_acc_list\n",
    "plt.ylim(0,1)\n",
    "'''\n",
    "#20\n",
    "plt.plot(x,train_acc_lists[0],label = '20 Train acc', color = \"crimson\")\n",
    "plt.plot(x,test_acc_lists[0],label = '20 Test acc', color = \"crimson\",linestyle = 'dashed')\n",
    "\n",
    "#30\n",
    "plt.plot(x,train_acc_lists[1],label = '30 Train acc', color = \"darkblue\")\n",
    "plt.plot(x,test_acc_lists[1],label = '30 Test acc', color = \"darkblue\",linestyle = 'dashed')\n",
    "\n",
    "#40\n",
    "plt.plot(x,train_acc_lists[2],label = '40 Train acc', color = \"green\")\n",
    "plt.plot(x,test_acc_lists[2],label = '40 Test acc', color = \"green\",linestyle = 'dashed')\n",
    "'''\n",
    "plt.plot(x,y1,label='train acc')\n",
    "plt.plot(x,y2,label='test acc')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Training and Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4065, 0.4174, 0.4706, 0.412, 0.4171, 0.41573333333333334, 0.41586666666666666, 0.4169333333333333, 0.4116666666666667, 0.41386666666666666, 0.4119, 0.4167, 0.4145, 0.4161666666666667, 0.4157666666666667, 0.4141]\n",
      "[0.3377, 0.3246, 0.2941, 0.3054, 0.2995, 0.298, 0.3004, 0.3012, 0.2941, 0.2941, 0.3006, 0.2993, 0.3011, 0.2949, 0.3034, 0.2941]\n"
     ]
    }
   ],
   "source": [
    "print(save_train_acc)\n",
    "print(save_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x*x-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_x_value = 5\n",
    "ini_x_tensor = ini_x_value*torch.ones(1, 1, dtype = torch.float64)\n",
    "x = Variable(ini_x_tensor, requires_grad=True)\n",
    "print(f'roop {0:<4d} x = {x.item()}')\n",
    "\n",
    "roop = 0\n",
    "while roop < 10:\n",
    "    roop += 1\n",
    "    # 勾配の計算\n",
    "    f(x).backward()\n",
    "    # xの更新\n",
    "    x.data -= (f(x)/x.grad).data\n",
    "    # 勾配を0に設定\n",
    "    x.grad.zero_()\n",
    "    print(f'roop {roop:<4d} x = {x.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なもの\n",
    "lossの履歴  \n",
    "trainのacc  \n",
    "testのacc  \n",
    "実行時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logs = []\n",
    "train_acc_logs = []\n",
    "test_acc_logs = []\n",
    "run_time_logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-16-32-64-128-128-64-32-16-10\n",
    "#8-64-512-512-64-10\n",
    "#8-96-1152-1152-500-150-20-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク設定\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN1,self).__init__()\n",
    "        self.conv1 = GraphConv(8,64)\n",
    "        self.conv2 = GraphConv(64,512)\n",
    "    \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.liner1 = torch.nn.Linear(512,64)\n",
    "        self.liner2 = torch.nn.Linear(64,10)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p = 0.2)\n",
    "\n",
    "    def forward(self,g,n_feat,e_feat = None):\n",
    "        h = F.relu(self.conv1(g,n_feat,None,e_feat))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(g,h,None,e_feat)\n",
    "        \n",
    "        h = self.flatten(h)\n",
    "\n",
    "        h = F.relu(self.liner1(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.liner2(h)\n",
    "        \n",
    "        g.ndata['h'] = h\n",
    "\n",
    "        return dgl.mean_nodes(g,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク設定\n",
    "class GCN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN2,self).__init__()\n",
    "        self.conv1 = GraphConv(8,96)\n",
    "        self.conv2 = GraphConv(96,1152)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.liner1 = torch.nn.Linear(1152,500)\n",
    "        self.liner2 = torch.nn.Linear(500,150)\n",
    "        self.liner3 = torch.nn.Linear(150,20)\n",
    "        self.liner4 = torch.nn.Linear(20,10)\n",
    "        self.dropout = torch.nn.Dropout(p = 0.2)\n",
    "\n",
    "    def forward(self,g,n_feat,e_feat = None):\n",
    "        h = F.relu(self.conv1(g,n_feat,None,e_feat))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(g,h,None,e_feat)\n",
    "        \n",
    "        h = self.flatten(h)\n",
    "\n",
    "        h = F.relu(self.liner1(h))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.liner2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.liner3(h)\n",
    "        \n",
    "        g.ndata['h'] = h\n",
    "\n",
    "        return dgl.mean_nodes(g,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_list = []\n",
    "opt_list = []\n",
    "model1 = GCN1()\n",
    "model_list.append(model1)\n",
    "model2 = GCN2()\n",
    "model_list.append(model2)\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "optimizer1 = optim.Adam(model1.parameters(),lr = 0.01)\n",
    "opt_list.append(optimizer1)\n",
    "optimizer2 = optim.Adam(model2.parameters(),lr = 0.01)\n",
    "opt_list.append(optimizer2)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_list)):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    start = time.time()\n",
    "    runmodel = model_list[i]\n",
    "    opt = opt_list[i]\n",
    "    runmodel.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batched_graph,labels in traindataloader:\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = runmodel(batched_graph, batched_graph.ndata['feat value'].float(),batched_graph.edata['distance'].float())\n",
    "            loss = F.cross_entropy(pred,labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "            num_tests += len(labels)\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(num_correct / num_tests)\n",
    "    loss_logs.append(loss_list)\n",
    "    run_time_logs.append(time.time() - start)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    runmodel.eval()\n",
    "    for batched_graph,labels in traindataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = runmodel(batched_graph, batched_graph.ndata['feat value'].float(),batched_graph.edata['distance'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    train_acc_logs.append(num_correct / num_tests)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    for batched_graph,labels in testdataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = runmodel(batched_graph, batched_graph.ndata['feat value'].float(),batched_graph.edata['distance'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    test_acc_logs.append(num_correct / num_tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_logs)\n",
    "print(train_acc_logs)\n",
    "print(test_acc_logs)\n",
    "print(run_time_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlog = model_list[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlog.__sizeof__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
    "#optimizer = optim.SGD(params=model.parameters(),lr=0.001,momentum=0.9)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
    "#optimizer = optim.SGD(params=model.parameters(),lr=0.001,momentum=0.9)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6])\n",
    "x_label = ['5','10','20','30','40','50']\n",
    "normal_Train_acc = [0.4089,0.3904,0.4532,0.4585,0.459,0.]\n",
    "normal_Test_acc = [0.2325,0.281,0.285,0.3005,0.302,0.]\n",
    "incpos_Train_acc = [0.4575,0.48,0.4853,0.4418,0.4774,0.]\n",
    "incpos_Test_acc = [0.2375,0.294,0.3035,0.34,0.3345,0.]\n",
    "std_Train_acc = [0.5268,0.5211,0.5501,0.5672,0.5776,0.5802]\n",
    "std_Test_acc = [0.2125,0.244,0.313,0.3255,0.364,0.3715]\n",
    "Train_data = [normal_Train_acc,incpos_Train_acc,std_Train_acc]\n",
    "Test_data = [normal_Test_acc,incpos_Test_acc,std_Test_acc]\n",
    "margin = 0.2\n",
    "totoal_width = 1 - margin\n",
    "\n",
    "for i,h in enumerate(Train_data):\n",
    "    pos = x - totoal_width *( 1- (2*i+1)/len(Train_data) )/2\n",
    "    plt.bar(pos, h, width = totoal_width/len(Train_data))\n",
    "plt.xticks(x,x_label)\n",
    "plt.title('Training acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(Train_data):\n",
    "    pos = x - totoal_width *( 1- (2*i+1)/len(Test_data) )/2\n",
    "    plt.bar(pos, h, width = totoal_width/len(Test_data))\n",
    "plt.xticks(x,x_label)\n",
    "plt.title('Test acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmodel = GCN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torchmodel.to(device)\n",
    "optimizer = optim.Adam(torchmodel.parameters(),lr = 0.001)\n",
    "epochs = 15\n",
    "\n",
    "history = {'train_loss':[],'train_acc':[],'test_acc':[]}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for e in range(epochs):\n",
    "    torchmodel.train()\n",
    "    loss = None\n",
    "\n",
    "    for i,(batched_graph, labels) in enumerate(traindataloader):\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = torchmodel(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "        loss = F.cross_entropy(pred,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if (i+1) % 10 == 0:\n",
    "        #    print(f'Training log: {e+1} epoch ({(i+1)*200} / 10000 train. data). Loss: {loss.item()}')\n",
    "\n",
    "    history['train_loss'].append(loss.item())\n",
    "\n",
    "    torchmodel.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(batched_graph, labels) in enumerate(tqdm(traindataloader)):\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = torchmodel(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "            correct += (pred.argmax(1) == labels).sum().item()\n",
    "            num_tests += len(labels)\n",
    "\n",
    "    acc = float(correct/num_tests)\n",
    "    history['train_acc'].append(acc)\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(batched_graph, labels) in enumerate(tqdm(testdataloader)):\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = torchmodel(batched_graph, batched_graph.ndata['feat value'].float())\n",
    "            correct += (pred.argmax(1) == labels).sum().item()\n",
    "            num_tests += len(labels)\n",
    "\n",
    "    acc = float(correct/num_tests)\n",
    "    history['test_acc'].append(acc)\n",
    "\n",
    "max_train_acc = max(history['train_acc'])\n",
    "min_train_loss = min(history['train_loss'])\n",
    "max_test_acc = max(history['test_acc'])\n",
    "\n",
    "print(f'Max train accuracy: {max_train_acc}')\n",
    "print(f'Min train loss: {min_train_loss}')\n",
    "print(f'Max test acc: {max_test_acc}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bae489d056b3b2fb3da90055ea9058b18f6663cd5fc5b4a870a71c1d277c079c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('DGL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9512f24219e729098bc0f30caa9285407db90c80b780d5e3a7fd78ad6fa2427c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
